   55  cat ws5.txt
   56  vi script.sh
   57  mkdir -p "Workhsheet 5/United States" && music_count=0 && entertainment_count=0 && gaming_count=0 && comedy_count=0 && while IFS=, read -r channel category country; do [ "$country" = "United States" ] && case "$category" in "Music") ((music_count++)) ;; "Entertainment") ((entertainment_count++)) ;; "Gaming") ((gaming_count++)) ;; "Comedy") ((comedy_count++)) ;; esac; done < "Global YouTube Statistics.csv" && echo "Music: $music_count" > ws5.txt && echo "Entertainment: $entertainment_count" >> ws5.txt && echo "Gaming: $gaming_count" >> ws5.txt && echo "Comedy: $comedy_count" >> ws5.txt
   58  cat ws5.txt
   59  mkdir -p "Workhsheet 5/United States" && music_count=0 && entertainment_count=0 && gaming_count=0 && comedy_count=0 && while IFS=, read -r channel category country; do [ "$country" = "United States" ] && case "$category" in "Music") ((music_count++)) ;; "Entertainment") ((entertainment_count++)) ;; "Gaming") ((gaming_count++)) ;; "Comedy") ((comedy_count++)) ;; esac; done < "Global YouTube Statistics.csv" && echo "Music: $music_count" > ws5.txt && echo "Entertainment: $entertainment_count" >> ws5.txt && echo "Gaming: $gaming_count" >> ws5.txt && echo "Comedy: $comedy_count" >> ws5.txt
   60  mkdir -p "Workhsheet 5/United States" && music_count=0 && entertainment_count=0 && gaming_count=0 && comedy_count=0 && while IFS=, read -r channel category country; do [ "$country" = "United States" ] && case "$category" in "Music") ((music_count++)) ;; "Entertainment") ((entertainment_count++)) ;; "Gaming") ((gaming_count++)) ;; "Comedy") ((comedy_count++)) ;; esac; done < "Global YouTube Statistics.csv" && echo "Music: $music_count" > ws55.txt && echo "Entertainment: $entertainment_count" >> ws55.txt && echo "Gaming: $gaming_count" >> ws55.txt && echo "Comedy: $comedy_count" >> ws55.txt
   61  cat ws55.txt
   62  ls
   63  categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do awk -F, -v category="$category" '$3 == "United States" && $4 == category {print $0}' "Global YouTube Statistics.csv" > "Workhsheet 5/United States/$category.txt"; done; wc -l "Workhsheet 5/United States/Music.txt" "Workhsheet 5/United States/Entertainment.txt" "Workhsheet 5/United States/Gaming.txt" "Workhsheet 5/United States/Comedy.txt" > ws5.txt
   64  cat ws5.txt
   65  categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do awk -F, -v category="$category" '$3 == "United States" && $4 == category {print $0}' 'Global YouTube Statistics.csv' > "Workhsheet 5/United States/$category.txt"; done; wc -l "Workhsheet 5/United States/Music.txt" "Workhsheet 5/United States/Entertainment.txt" "Workhsheet 5/United States/Gaming.txt" "Workhsheet 5/United States/Comedy.txt" > ws5.txt
   66  cat ws5.txt
   67  ls
   68  categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do awk -F, -v category="$category" '$3 == "United States" && $4 == category {print $0}' 'Global YouTube Statistics.csv' > "Worksheet 5/United States/$category.txt"; done; wc -l "Worksheet 5/United States/Music.txt" "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" > ws5.txt
   69  categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do awk -F, -v category="$category" '$3 == "United States" && $4 == category {print $0}' 'Global YouTube Statistics.csv' > "Workhsheet 5/United States/$category.txt"; done; wc -l "Workhsheet 5/United States/Music.txt" "Workhsheet 5/United States/Entertainment.txt" "Workhsheet 5/United States/Gaming.txt" "Workhsheet 5/United States/Comedy.txt" > ws5.txt
   70  cat ws5.txt
   71  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/
   72  United States/${category}.txt"; done
   73  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/; United States/${category}.txt"; done
   74  exit
   75  exit
   76  quit
   77  exit
   78  exit
   79  eixt
   80  eixt
   81  exit
   82  "wowo"
   83  'exit'
   84  exit
   85  exit
   86  exit
   87  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/
   88  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/United States/${category}.txt"; done
   89  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/United States/${category}.txt"; done
   90  cd cs131
   91  cd Worksheet5
   92  vi ws5.txt
   93  input_file="Global YouTube Statistics.csv"
   94  output_directory="Workhsheet 5/United States"
   95  mkdir -p "$output_directory"
   96  categories=("Music" "Entertainment" "Gaming" "Comedy")
   97  for category in "${categories[@]}"; do     touch "$output_directory/$category.txt"; done
   98  count_and_write_to_ws5() {     category_name="$1";     category_file="$output_directory/$category_name.txt";     entry_count=$(wc -l < "$category_file");     echo "$category_name: $entry_count" >> "ws5.txt"; }
   99  IFS=$'\n'
  100  while read -r line; do
  101      IFS=',' read -r -a fields <<< "$line";      if [[ "${fields[3]}" == "United States" ]]; then         for category in "${categories[@]}"; do             if [[ "${fields[4]}" == "$category" ]]; then                 echo "$line" >> "$output_directory/$category.txt";             fi;         done;     fi; done < "$input_file"
  102  for category in "${categories[@]}"; do     count_and_write_to_ws5 "$category"; done
  103  vi script.sh
  104  input_file="Global YouTube Statistics.csv"; output_directory="Workhsheet 5/United States"; mkdir -p "$output_directory"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do touch "$output_directory/$category.txt"; done; count_and_write_to_ws5() { category_name="$1"; category_file="$output_directory/$category_name.txt"; entry_count=$(wc -l < "$category_file"); echo "$category_name: $entry_count" >> "ws5.txt"; }; IFS=$'\n'; while read -r line; do IFS=',' read -r -a fields <<< "$line"; if [[ "${fields[3]}" == "United States" ]]; then for category in "${categories[@]}"; do if [[ "${fields[4]}" == "$category" ]]; then echo "$line" >> "$output_directory/$category.txt"; fi; done; fi; done < "$input_file"; for category in "${categories[@]}"; do count_and_write_to_ws5 "$category"; done
  105  cat ws5.txt
  106  ls
  107  cat 'Global YouTube Statistics.csv'
  108  grep Music
  109  touch ws5.txt
  110  input_file="Global YouTube Statistics.csv"; output_directory="Workhsheet 5/United States"; mkdir -p "$output_directory"; touch "ws5.txt"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do touch "$output_directory/$category.txt"; done; count_and_write_to_ws5() { category_name="$1"; category_file="$output_directory/$category_name.txt"; entry_count=$(wc -l < "$category_file"); echo "$category_name: $entry_count" >> "ws5.txt"; }; IFS=$'\n'; while read -r line; do IFS=',' read -r -a fields <<< "$line"; if [[ "${fields[3]}" == "United States" ]]; then for category in "${categories[@]}"; do if [[ "${fields[4]}" == "$category" ]]; then echo "$line" >> "$output_directory/$category.txt"; fi; done; fi; done < "$input_file"; for category in "${categories[@]}"; do count_and_write_to_ws5 "$category"; done
  111  cat ws5.txt
  112  vi ws5.txt
  113  cat ws5.txt
  114  input_file="Global YouTube Statistics.csv"; output_directory="Workhsheet 5/United States"; mkdir -p "$output_directory"; touch "ws5.txt"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do touch "$output_directory/$category.txt"; done; count_and_write_to_ws5() { category_name="$1"; category_file="$output_directory/$category_name.txt"; entry_count=$(wc -l < "$category_file"); echo "$category_name: $entry_count" >> "ws5.txt"; }; IFS=$'\n'; while read -r line; do IFS=',' read -r -a fields <<< "$line"; if [[ "${fields[3]}" == "United States" ]]; then for category in "${categories[@]}"; do if [[ "${fields[4]}" == "$category" ]]; then echo "$line" >> "$output_directory/$category.txt"; fi; done; fi; done < "$input_file"; for category in "${categories[@]}"; do count_and_write_to_ws5 "$category"; done
  115  cat ws5.txt
  116  input_file="Global YouTube Statistics.csv"; output_directory="Workhsheet 5/United States"; mkdir -p "$output_directory"; touch "ws5.txt"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do touch "$output_directory/$category.txt"; done; IFS=$'\n'; cat "$input_file" | while read -r line; do IFS=',' read -r -a fields <<< "$line"; if [ "${fields[3]}" = "United States" ]; then for category in "${categories[@]}"; do if [ "${fields[4]}" = "$category" ]; then echo "$line" >> "$output_directory/$category.txt"; fi; done; fi; done; for category in "${categories[@]}"; do echo "$category: $(cat "$output_directory/$category.txt" | wc -l)" >> "ws5.txt"; done
  117  cat ws5.txt
  118  input_file="Global YouTube Statistics.csv"; output_directory="Workhsheet 5/United States"; mkdir -p "$output_directory"; touch "results.txt"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do touch "$output_directory/$category.txt"; done; IFS=$'\n'; cat "$input_file" | while read -r line; do IFS=',' read -r -a fields <<< "$line"; if [ "${fields[3]}" = "United States" ]; then for category in "${categories[@]}"; do if [ "${fields[4]}" = "$category" ]; then echo "$line" >> "$output_directory/$category.txt"; fi; done; fi; done; for category in "${categories[@]}"; do echo "$category: $(cat "$output_directory/$category.txt" | wc -l)" >> "results.txt"; done
  119  cat results.txt
  120  mkdir -p "Worksheet 5/United Stat es' && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@1}"; do grep "United States, .*category" "Global YouTube Statistics.cv' >> "Workhsheet 5/ United States/${category}.txt"; done
  121  mkdir -p "Worksheet 5/Untied States' 
  122  mkdir -p "Worksheet 5/Untied States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global Youtube Statistics.csv" >> Worksheet 5/United States/${category}.txt"; done
  123  ls
  124  mkdir -p "Worksheet 5/Untied States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global Youtube Statistics.csv" >> Worksheet 5/United States/${category}.txt"; done
  125  mkdir -p "Worksheet 5/Untied States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global Youtube Statistics.csv" >> Worksheet 5/United States/${category}.txt"; done 
  126  vi script.sh
  127  ./script.sh
  128  vi script.sh
  129  mkdir -p "Worksheet 5/Untied States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global Youtube Statistics.csv" >> "Worksheet 5/United States/${category}.txt"; done
  130  mkdir -p "Worksheet 5/United States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global Youtube Statistics.csv" >> "Worksheet 5/United States/${category}.txt"; done
  131  ls
  132  mkdir -p "Worksheet 5/Untied States" && categories=("Music" "Entertainment" "Gaming" "Comedy") && for category in "${categories[@]}"; do grep "United States,.*$category" "Global YouTube Statistics.csv" >> "Worksheet 5/United States/${category}.txt"; done
  133  cat ws5.txt
  134  wc -l "Worksheet5/United_States/Music.txt" "Worksheet5/United_States/Entertainment.txt" "Worksheet5/United_States/Gaming.txt" "Worksheet5/United_States/Comedy.txt" > ws5.txt
  135  wc -w "Worksheet 5/United States/Music.txt" "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" > ws5.txt
  136  cat ws5.txt
  137  vi script.sh
  138  ./script.sh
  139  cat ws5.txt
  140  input_file="Global YouTube Statistics.csv"; mkdir -p "Worksheet 5/United States"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do count=0; while IFS=',' read -r col1 col2 col3 col4 col5; do if [ "$col4" == "United States" ] && [ "$col5" == "$category" ]; then echo "$col1,$col2,$col3,$col4,$col5" >> "Worksheet 5/United States/$category.txt"; count=$((count+1)); fi; done < "$input_file"; echo "Number of entries in Worksheet 5/United States/$category.txt: $count" >> "ws5.txt"; done
  141  cat ws5.txt
  142  awk -F',' 'NR==1 {for (i=1; i<=NF; i++) if ($i == "Entertainment") print "Column " i " is Entertainment"}' "Global YouTube Statistics.csv"
  143  head -n 1 "Global YouTube Statistics.csv" | tr ',' '\n' | cat -n | grep "Entertainment"
  144  head -n 1 "Global YouTube Statistics.csv" | tr ',' '\n'| grep "Entertainment"
  145  head -n 1 'Global YouTube Statistics.csv' | tr ',' '\n' | grep -n 'highest_monthly_earnings'
  146  head -n 1 'Global YouTube Statistics.csv' | tr ',' '\n' | grep -n 'Entertainment
  147  head -n 1 'Global YouTube Statistics.csv' | tr ',' '\n' | grep -n 'Entertainment'
  148  input_file="Global YouTube Statistics.csv"; mkdir -p "Worksheet 5/United States"; categories=("Music" "Entertainment" "Gaming" "Comedy"); for category in "${categories[@]}"; do count=0; while IFS=',' read -r col1 col2 col3 col4 col5; do if [ "$col4" == "United States" ] && [ "$col5" == "$category" ]; then echo "$col1,$col2,$col3,$col4,$col5" >> "Worksheet 5/United States/$category.txt"; count=$((count+1)); fi; done < "$input_file"; echo "Number of entries in Worksheet 5/United States/$category.txt: $count" >> "ws5.txt"; done
  149  cat ws5.txt
  150  vi script.sh
  151  ./script.sh
  152  vi script.sh
  153  categories=("Music" "Entertainment" "Gaming" "Comedy"); while IFS=, read -r rank youtuber subscribers video_views category title uploads country abbreviation channel_type video_views_rank country_rank channel_type_rank video_views_for_the_last_30_days lowest_monthly_earnings highest_monthly_earnings lowest_yearly_earnings highest_yearly_earnings subscribers_for_last_30_days created_year created_month created_date gross_tertiary_education_enrollment population unemployment_rate urban_population latitude longitude; do if [[ "$country" == "United States" ]]; then echo "$rank,$youtuber,$subscribers,$video_views,$category,$title,$uploads,$country,$abbreviation,$channel_type,$video_views_rank,$country_rank,$channel_type_rank,$video_views_for_the_last_30_days,$lowest_monthly_earnings,$highest_monthly_earnings,$lowest_yearly_earnings,$highest_yearly_earnings,$subscribers_for_last_30_days,$created_year,$created_month,$created_date,$gross_tertiary_education_enrollment,$population,$unemployment_rate,$urban_population,$latitude,$longitude" >> "Worksheet 5/United States/${category}.txt"; fi; done < "Global YouTube Statistics.csv"; wc -w "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" "Worksheet 5/United States/Music.txt" >> ws5.txt
  154  cat ws5.txt
  155  cat "Worksheet 5/United States/Gaming.txt"
  156  history > cmds.log
  157  ls
  158  cd cs131
  159  cd Worksheet5
  160  ./script.sh
  161  vi script.sh
  162  cp script.sh oldscript.sh
  163  cat oldscript.sh
  164  vi script.sh
  165  cat script.sh
  166  chmod +x script.sh
  167  ./script.sh
  168  cat ws5.txt
  169  categories=("Music" "Entertainment" "Gaming" "Comedy"); while IFS=, read -r rank youtuber subscribers video_views category title uploads country abbreviation channel_type video_views_rank country_rank channel_type_rank video_views_for_the_last_30_days lowest_monthly_earnings highest_monthly_earnings lowest_yearly_earnings highest_yearly_earnings subscribers_for_last_30_days created_year created_month created_date gross_tertiary_education_enrollment population unemployment_rate urban_population latitude longitude; do if [[ "$country" == "United States" ]]; then echo "$rank,$youtuber,$subscribers,$video_views,$category,$title,$uploads,$country,$abbreviation,$channel_type,$video_views_rank,$country_rank,$channel_type_rank,$video_views_for_the_last_30_days,$lowest_monthly_earnings,$highest_monthly_earnings,$lowest_yearly_earnings,$highest_yearly_earnings,$subscribers_for_last_30_days,$created_year,$created_month,$created_date,$gross_tertiary_education_enrollment,$population,$unemployment_rate,$urban_population,$latitude,$longitude" >> "Worksheet 5/United States/${category}.txt"; fi; done < "Global YouTube Statistics.csv"; wc -w "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" "Worksheet 5/United States/Music.txt" >> ws5.txt
  170  cat script.sh
  171  ./script.sh
  172  vi script.sh
  173  ./script.sh
  174  cat ws5.txt
  175  cat "Worksheet 5/United States/Entertainment.txt"
  176  categories=("Music" "Entertainment" "Gaming" "Comedy"); while IFS=, read -r rank youtuber subscribers video_views category title uploads country abbreviation channel_type video_views_rank country_rank channel_type_rank video_views_for_the_last_30_days lowest_monthly_earnings highest_monthly_earnings lowest_yearly_earnings highest_yearly_earnings subscribers_for_last_30_days created_year created_month created_date gross_tertiary_education_enrollment population unemployment_rate urban_population latitude longitude; do if [[ "$country" == "United States" ]]; then echo "$rank,$youtuber,$subscribers,$video_views,$category,$title,$uploads,$country,$abbreviation,$channel_type,$video_views_rank,$country_rank,$channel_type_rank,$video_views_for_the_last_30_days,$lowest_monthly_earnings,$highest_monthly_earnings,$lowest_yearly_earnings,$highest_yearly_earnings,$subscribers_for_last_30_days,$created_year,$created_month,$created_date,$gross_tertiary_education_enrollment,$population,$unemployment_rate,$urban_population,$latitude,$longitude" >> "Worksheet 5/United States/${category}.txt"; fi; done < "Global YouTube Statistics.csv"; wc -w "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" "Worksheet 5/United States/Music.txt" >> ws5.txt
  177  cat ws5/txt
  178  cat ws5.txt
  179  vi script.sh
  180  ./script.sh
  181  vi ws5.txt
  182  cat ws5.txt
  183  cat script.txt
  184  cat script.sh
  185  ls
  186  rmdir 'Worksheet 5'
  187  rm -r 'Worksheet 5'
  188  ./script.sh
  189  mkdir -p "Workhsheet 5/United States"
  190  ./script
  191  ./script.sh
  192  ls
  193  categories=("Music" "Entertainment" "Gaming" "Comedy"); while IFS=, read -r rank youtuber subscribers video_views category title uploads country abbreviation channel_type video_views_rank country_rank channel_type_rank video_views_for_the_last_30_days lowest_monthly_earnings highest_monthly_earnings lowest_yearly_earnings highest_yearly_earnings subscribers_for_last_30_days created_year created_month created_date gross_tertiary_education_enrollment population unemployment_rate urban_population latitude longitude; do if [[ "$country" == "United States" ]]; then echo "$rank,$youtuber,$subscribers,$video_views,$category,$title,$uploads,$country,$abbreviation,$channel_type,$video_views_rank,$country_rank,$channel_type_rank,$video_views_for_the_last_30_days,$lowest_monthly_earnings,$highest_monthly_earnings,$lowest_yearly_earnings,$highest_yearly_earnings,$subscribers_for_last_30_days,$created_year,$created_month,$created_date,$gross_tertiary_education_enrollment,$population,$unemployment_rate,$urban_population,$latitude,$longitude" >> "Worksheet 5/United States/${category}.txt"; fi; done < "Global YouTube Statistics.csv"; wc -w "Worksheet 5/United States/Entertainment.txt" "Worksheet 5/United States/Gaming.txt" "Worksheet 5/United States/Comedy.txt" "Worksheet 5/United States/Music.txt" >> ws5.txt
  194  mkdir -p "Worksheet 5/United States"
  195  ls
  196  ls -l "Worksheet 5/United States"
  197  ./script.sh
  198  ws5.txt
  199  cat ws5.txt
  200  cd ..
  201  git add 
  202  git add .
  203  git add Worksheet5
  204  git status
  205  cd Worksheet5
  206  git init
  207  git commit -m"Worksheet5"
  208  git add
  209  git add .
  210  git init
  211  git push origin master
  212  git push -u origin master
  213  history > cmds.log
  214  git push origin master
  215  git push master
  216  git push master origin
  217  git status
  218  git add .
  219  cd ..
  220  git add Worksheet5
  221  cd Worksheet5
  222  git status
  223  git push --set-upstream origin master
  224  git branch
  225  git checkout -b master
  226  git checkout master
  227  git add .
  228  git commit -m "Initial commit"
  229  git push --set-upstream origin master
  230  git commit -m "Your commit message here"
  231  git push
  232  git push master origin
  233  git push origin master
  234  git push
  235  git checkout -b origin
  236  git push master origin
  237  git push --set-upstream origin master
  238  git commit -m "Message"
  239  git push --set-upstream origin master
  240  Please make sure you have the correct access rights
  241  and the repository exists.
  242  [hamzaf23@sjsu Worksheet5]$ 
  243  [hamzaf23@sjsu Worksheet5]$ git push --set-upstream origin master
  244  fatal: 'origin' does not appear to be a git repository
  245  fatal: Could not read from remote repository.
  246  Please make sure you have the correct access rights
  247  and the repository exists.Please make sure you have the correct access rights
  248  and the repository exists.
  249  [hamzaf23@sjsu Worksheet5]$ 
  250  [hamzaf23@sjsu Worksheet5]$ git push --set-upstream origin master
  251  fatal: 'origin' does not appear to be a git repository
  252  fatal: Could not read from remote repository.
  253  Please make sure you have the correct access rights
  254  and the repository exists.
  255  git remote add origin https://github.com/HKcode22/cs131
  256  git push --set-upstream origin master
  257  SHA256:qrEPJjVToUhzA65CLsJekd496eQWIZv9bUl2ZE912R4
  258  git push --set-upstream origin master
  259  git fetch origin
  260  git merge origin/master
  261  git merge --allow-unrelated-histories origin/master
  262  git push
  263  git push --set-upstream origin origin
  264  git add
  265  git add .
  266  git init
  267  git status
  268  cd ..
  269  git add Worksheet5
  270  cd ..
  271  cd cs131
  272  cd Worksheet5
  273  git push --set-upstream origin origin
  274  cd ..
  275  git push --set-upstream origin origi
  276  git push --set-upstream origin origin
  277  git push --set-upstream origin
  278  git push --set-upstream origin origi
  279  git push --set-upstream origin
  280  git push origin master
  281  cd Worksheet5
  282  git push origin master
  283  ws5.txt
  284  cat ws5.txt
  285  git push --force origin master
  286  cd ..
  287  git push --force origin master
  288  ls
  289  git add Worksheet5
  290  git add ,
  291  git add .
  292  git status
  293  git push --force origin master
  294  cd Wokrshett5
  295  cd Worksheet5
  296  git commit -m"add"
  297  git push origin master
  298  cd ..
  299  git push origin master
  300  git remote set-url origin git@github.com:HKcode22/cs131.git
  301  git push origin master
  302  git add .
  303  git add Worksheet5
  304  ls
  305  git push origin master
  306  cd Worksheet5
  307  git commit -m"add"
  308  git add .
  309  git commit -m"add"
  310  git push origin master
  311  git push --force origin master
  312  cd ..
  313  git push origin
  314  git push origin master
  315  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
  316  git push --force origin master
  317  ls
  318  git add Worksheet5
  319  mkdir Worksheet0
  320  git push --force origin master
  321  git add .
  322  git add Worksheet0
  323  git push --force origin master
  324  git push orign master
  325  git push --set-upstream origin master
  326  git commit -m "Message"
  327  git push --set-upstream origin master
  328  ls
  329  cd Worksheet5
  330  rm -r Wokrhseet1
  331  ls
  332  rm -r Worksheet1
  333  rm -r Worksheet2 | rm -r Worksheet4
  334  ls
  335  rm -r Assignment1 | rm -r Assignment2 | rm -r Worksheet3 | rm -r sampleProject
  336  ls
  337  git -m"add"
  338  git commit -m"add"
  339  cd ..
  340  git add .
  341  git add WOrksheet5
  342  git add Worksheet5
  343  git commit -m"message"
  344  git push --set-upstream origin master
  345  cd cs131
  346  mkdir Assignment3
  347  wget https://raw.githubusercontent.com/khanchandaniashish/CS131/main/awkdata/last.fake
  348  cd Assignment3
  349  wget https://raw.githubusercontent.com/khanchandaniashish/CS131/main/awkdata/last.fake
  350  wget https://github.com/khanchandaniashish/CS131/blob/main/awkdata/passwd.fake 
  351  wget https://github.com/khanchandaniashish/CS131/blob/main/awkdata/ps.fake 
  352  ls
  353  wc -l last.fake
  354  grep 'f23' last.fake > lastf23.fake
  355  wc -l lastf23.fake
  356  grep 'Sun' lastf23.fake | awk '{print $1}' | sort -u
  357  awk '$NF >= 23 || $NF <= 4' lastf23.fake | awk '{print $1}' | sort -u
  358  awk '$NF >= 5 && $NF <= 9' lastf23.fake | awk '{print $1}' | sort -u
  359  awk '$1 ~ /^t/' lastf23.fake
  360  awk -F: '$1 ~ /^a/ && $3 % 2 == 0' passwd.fake
  361  sed -e 's/[a-zA-Z0-9 ]//g' passwd.fake | sort -u
  362  awk -F: -v OFS="\t" '$1 ~ /^a/ && $3 % 2 == 0' passwd.fake
  363  wc -l last.fake
  364  grep 'f23' last.fake > lastf23.fake
  365  wc -l lastf23.fake
  366  grep 'Sun' lastf23.fake | awk '{print $1}' | sort | uniq
  367  awk '$NF ~ /23$/ && $NF >= 11 && $NF <= 4 {print $1}' lastf23.fake | sort | uniq
  368  awk '$3 == "Sun" {print $1}' lastf23.fake | sort | uniq
  369  awk '$NF ~ /23$/ && $NF >= 11 && $NF <= 4 {print $1}' lastf23.fake | sort | uniq
  370  awk -F' ' '{split($6, a, ":"); if ((a[1] >= 23 && a[2] >= 0) || (a[1] <= 4 && a[2] >= 0)) print $1}' lastf23.fake | sort | uniq
  371  awk '$NF ~ /23$/ && $NF >= 5 && $NF <= 9 {print $1}' lastf23.fake | sort | uniq
  372  awk -F' ' '{split($6, a, ":"); if ((a[1] >= 5 && a[2] >= 0) && (a[1] <= 9 && a[2] >= 0)) print $1}' lastf23.fake | sort | uniq
  373  awk '$1 ~ /^t/ {print $1}' lastf23.fake
  374  awk -F: '$1 ~ /^a/ && $3 % 2 == 0 {print $1}' passwd.fake
  375  awk -F: '$1 ~ /^a/ {print $1}' passwd.fake
  376  awk -F: '{print $1}' passwd.fake
  377  awk -F'{' '{print $1}' passwd.fake | grep -Eo 'a[^{]+' | awk '$1 % 2 == 0 {print}'
  378  cat passwd.fake
  379  awk -F'"' '$2 ~ /^a/ && $4 % 2 == 0 {print $2}' passwd.fake
  380  awk -F: '$1 ~ /^a/ && $3 % 2 == 0' passwd.fake
  381  awk -F: '$1 ~ /^a/ && $3 % 2 == 0 {print $0}' passwd.fake
  382  awk -F: '{ if ($1 ~ /^a/ && $3 % 2 == 0) print $0 }' passwd.fake
  383  awk -F: '$1 ~ /^a/ && $3 % 2 == 0 {print $1}' passwd.fake
  384  sed -E 's/[a-zA-Z0-9 ]//g' passwd.fake | sort -u
  385  cat passwd.fake
  386  grep "a" paawk -F: '$1 ~ /^a/' passwd.fake
  387  awk -F: '$1 ~ /^a/' passwd.fake
  388  grep '^a' passwd.fake
  389  man last
  390  awk '$4 ~ /Sun/ {print $1}' lastf23.fake | sort | uniq
  391  awk '$4 ~ /Sun/ {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} \
  392       {print $0} \
  393       END {print "Kudos to these people for putting in the work"}'
  394  awk '$4 ~ /Sun/ {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  395  awk '$5 ~ /11:/, $5 ~ /04:/{print $1}' lastf23.fake | sort | uniq
  396  awk '($5 >= "23:00" || $5 <= "04:00"){print $1}' lastf23.fake | sort | uniq
  397  awk '($5 >= "23:00" || $5 <= "04:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  398  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  399  h
  400  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  401  awk '($5 >= "05:00" && $5 <= "09:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Early Birds List:"} {print $0} END {print "Early Birds are dope. Please ensure you get enough sleep though."}'
  402  awk '$5 >= "05:00" && $5 <= "09:00" {print $1}' lastf23.fake | sort | uniq
  403  awk '$NF ~ /([5-9]:[0-5][0-9]:[0-5][0-9])|(09:00:00)/ {print $1}' last.fake | sort | uniq
  404  cat last.fake
  405  awk '$NF >= "05:00:00" && $NF <= "09:00:00" {print $1}' last.fake | sort | uniq
  406  awk '{split($10, a, ":"); if (a[1] >= 5 && a[1] <= 9) print $1}' last.fake | sort | uniq
  407  awk '$NF ~ /:[0-9][0-9]:[0-9][0-9]/ && $NF >= ":05:00" && $NF <= ":09:00" {print $1}' last.fake | sort | uniq
  408  grep -Eo '^[^[:space:]]+' last.fake | awk -F: '$1 >= 5 && $1 <= 9 {print $1}' | sort | uniq
  409  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  410  awk '($5 >= "05:00" || ($5 <= "09:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  411  awk '($5 >= "05:00" && ($5 <= "09:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  412  awk '($5 >= "05:00" && $5 <= "09:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Early Birds List:"} {print $0} END {print "Early Birds are dope. Please ensure you get enough sleep though."}'
  413  awk '$7 >= 5 && $7 <= 9 {print $1}' lastf23.fake | sort -u
  414  awk '{print $1}' lastf23.fake | sort -u
  415  cat lastf23.fake
  416  cat last.fake
  417  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  418  print $0
  419  awk '{print $1}' lastf23.fake | sort -u
  420  awk '{print $0}' lastf23.fake | sort -u
  421  awk '{print $2}' lastf23.fake | sort -u
  422  awk '{print $3}' lastf23.fake | sort -u
  423  awk '{print $4}' lastf23.fake | sort -u
  424  awk '{print $5}' lastf23.fake | sort -u
  425  awk '{print $6}' lastf23.fake | sort -u
  426  awk '{print $7}' lastf23.fake | sort -u
  427  awk '{print $8}' lastf23.fake | sort -u
  428  awk '{print $5}' lastf23.fake | sort -u
  429  awk '($6 >= "23:00" || ($6 >= "00:00" && $6 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  430  awk '($7 >= "23:00" || ($7 >= "00:00" && $7 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  431  awk '($7 >= "23:00" || ($7 >= "00:00" && $7 <= "04:00")){print $1}' lastf23.fake | sort | uniq
  432  awk '{print $1}' lastf23.fake | sort -u
  433  awk '$4 >= "10:00:00" && $4 <= "12:00:00" { print $1 }' logfile.txt
  434  awk '$2 >= "10:00" && $2 <= "12:00" { print $1 }' lastf23.fake | sort | uniq
  435  awk '$2 >= "10:00" && $2 <= "12:00" { print $1 }' lastf23.fake
  436  awk '$2 >= ("10:00" && $2 <= "12:00") { print $1 }' lastf23.fake
  437  awk '{print $0}' your_data_file
  438  cat lastf23.fake
  439  awk '($7 >= "23:00" && $8 <= "04:00") {print $1}' lastf23.fake | sort -u
  440  awk '($7 >= "05:00" && $8 <= "09:00") {print $1}' lastf23.fake | sort -u
  441  awk {print $7}
  442  awk '{print &7}'
  443  awk '{print $7}
  444  awk '{print &7}'
  445  awk '{print $7}'
  446  awk '{print $7}' lastf23.fake
  447  awk '{print $8}' lastf23.fake
  448  awk '{print $9}' lastf23.fake
  449  awk '($7 >= "23:00" && $9 <= "04:00") {print $1}' lastf23.fake | sort -u
  450  awk '($7 >= "23:00" && $8 <= "04:00") {print $1}' lastf23.fake | sort -u
  451  awk '($7 >= "05:00" && $9 <= "09:00") {print $1}' lastf23.fake | sort -u
  452  awk '($7 >= "05:00" && $8 <= "09:00") {print $1}' lastf23.fake | sort -u
  453  cd cs131
  454  cd Assignment3
  455  vi Assignment3.txt
  456  cd cs131
  457  cd Assignment3
  458  cd cs131
  459  cd Assignment3
  460  vi Assignment3.txt
  461  vi Assingment3.txt
  462  cat Assingmnt3.txt
  463  cat Assignment3.txt
  464  cat Assingmnt3.txt
  465  ls
  466  cat Assingment3.txt
  467  cp Assingment3.txt Assignment3.txt
  468  ls
  469  cat Assignment3.txt
  470  cd cs131
  471  cd ..
  472  git add .
  473  git add Assignment3
  474  git status
  475  cd Assignment3
  476  history > cmds.log
  477  ls
  478  cd ..
  479  git add .
  480  git add Assignment3
  481  git status
  482  cd Assignment3
  483  git commit -m"add"
  484  git push orgin master
  485  git push --set-upstream origin master
  486  cd cs131
  487  cd Assignment3
  488  awk '{print $1}' lastf23.fake | sort -u
  489  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  490  awk '($5 >= "23:00" || ($5 >= "00:00" && $5 <= "04:00")){print $1}' lastf23.fake | sort | uniq | awk 'BEGIwc -l last.fakeN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  491  wc -l last.fake
  492  grep 'f23' last.fake > lastf23.fake
  493  wc -l lastf23.fake
  494  cat lastf23.fake
  495  awk '$4=="Sun" {print $1}' lastf23.fake | sort | uniq
  496  awk '$4==Sun {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  497  '$4==Sun {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  498  The list of people who worked on'$4==Sun {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  499  The list of people who worked on
  500  awk '($4==Sun) {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  501  awk '($4=="Sun") {print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "The list of people who worked on Sunday are:"} {print $0} END {print "Kudos to these people for putting in the work"}'
  502  awk '($7 >= "23:00" && $8 <= "04:00") {print $1}' lastf23.fake | sort -uawk '($7 >= "23:00" && $8 <= "04:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  503  awk '($7 >= "23:00" && $8 <= "04:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Night Owls List:"} {print $0} END {print "Night Owls are dope. Please ensure you get enough sleep though."}'
  504  awk '($7 >= "05:00" && $8 <= "09:00"){print $1}' lastf23.fake | sort | uniq | awk 'BEGIN {print "Early Birds List:"} {print $0} END {print "Early Birds are dope. Please ensure you get enough sleep though."}'
  505  awk '$1 ~ /^t/' lastf23.fake | sort -u
  506  awk '$1 ~ /^a/ && $3 % 2 == 0' passwd.fake | sort -u
  507  awk '($1 ~ /^a/ && $3 % 2 == 0)' passwd.fake | sort -u
  508  xawk '($1 ~ /^a/ && $3 % 2 == 0)' passwd.fake | sort -u
  509  awk '($1==/^a/ && $3 % 2 == 0)' passwd.fake | sort -u
  510  awk '($1 ~ /^a/ && $3 % 2 == 0)' passwd.fake | sort -uawk -F: '$1 ~ /^a/ && $3 % 2 == 0 {print $1}' passwd.fake | sort -u
  511  awk -F: '$1 ~ /^a/ && $3 % 2 == 0 {print $1}' passwd.fake | sort -u
  512  awk -F: '($1 ~ /^a/) && ($3 % 2 == 0) {print $1}' passwd.fake | sort -u
  513  awk -F: '($1 ~ /^a/) && ($2 % 2 == 0) {print $1}' passwd.fake | sort -u
  514  sed 's/[a-zA-Z0-9 ]//g' passwd.fake | sort -u
  515  cd cs131
  516  ls
  517  mkdir MiniProjectGroup2
  518  cd MiniProjectGroup2
  519  https://www.kaggle.com/datasets/alexq1111/imdb-top-rated-english-movies/
  520  pip install kaggle
  521  wget how do i donwload https://www.kaggle.com/datasets/alexq1111/imdb-top-rated-english-movies/ to mac book terminal
  522  wget https://www.kaggle.com/datasets/alexq1111/imdb-top-rated-english-movies/
  523  ls
  524  cat index.html
  525  ls
  526  index.html.1
  527  cat index.html.1
  528  touch Names.txt
  529  echo -e "Abdul\nHamza\nRogelio" > Names.txt
  530  history > cmds.log
  531  cd ..
  532  git add .
  533  git add MiniPojectGroup2
  534  git add MiniProjectGroup2
  535  git status
  536  cd MiniProjectGroup2
  537  git commit -m"added"
  538  git push origin master
  539  cd cs131
  540  ls
  541  cd MiniProjectGroup2
  542  ls
  543  rm *.html
  544  ls
  545  rm index.html.1
  546  ls
  547  wget -O topmovies.csv
  548  wget topmovies.csv https://www.kaggle.com/datasets/omarhanyy/imdb-top-1000
  549  ls
  550  cat imdb-top-1000
  551  ls
  552  rm imdb-top-1000
  553  cat topmovies.csv
  554  cd ~/Downloads
  555  cat /path/to/your/local/Downloads/file.csv
  556  wget https://www.kaggle.com/omarhanyy/imdb-top-1000/raw/master/topmovies.csv
  557  ls
  558  wget https://www.kaggle.com/omarhanyy/imdb-top-1000/raw/master/topmovies.csv
  559  curl -O https://www.kaggle.com/omarhanyy/imdb-top-1000/raw/master/topmovies.csv
  560  ls
  561  cat topmovies.csv
  562  curl -O https://www.kaggle.com/omarhanyy/imdb-top-1000/raw/master/topmovies.csv
  563  ls
  564  curl -I https://www.kaggle.com/omarhanyy/imdb-top-1000/raw/master/topmovies.csv
  565  cat topmovies.csv
  566  pip install kaggle
  567  sudo apt-get update
  568  pip install kaggle
  569  wget -O "IMDB_top_1000.csv" "https://www.kaggle.com/omarhanyy/imdb-top-1000/download"
  570  ls
  571  cat IMDB_top_1000.csv
  572  wget --cookies=on --header "Cookie: _kaggle_session=$(curl -s -b cookies.txt https://www.kaggle.com/account/login?returnUrl=%2Fdatasets%2Fomarhanyy%2Fimdb-top-1000 | grep -o 'csrf_token" content="[a-z0-9]\+"' | grep -o '[a-z0-9]\+' | head -n 1)" -O "IMDB_top_1000.csv" "https://www.kaggle.com/omarhanyy/imdb-top-1000/download"
  573  .s
  574  ls
  575  cat IMDB_top_1000.csv
  576  curl -LJO https://www.kaggle.com/datasets/omarhanyy/imdb-top-1000/download/IMDB%20top%201000.csv
  577  wget --content-disposition https://www.kaggle.com/datasets/omarhanyy/imdb-top-1000/download/IMDB%20top%201000.csv
  578  ls
  579  cat IMDB%20top%201000.csv
  580  ls
  581  cat IMDB_top_1000.csv
  582  ls
  583  pip install kaggle
  584  kaggle login
  585  pip3 install kaggle
  586  kaggle login
  587  pip install kaggle
  588  pip3 install --upgrade pip
  589  pip3 install --force-reinstall kaggle
  590  pip install kaggle
  591  kaggle login
  592  brew install python3
  593  pip3 install kaggle
  594  python3 -m kaggle
  595  python -m kaggle
  596  kaggle login
  597  export PATH="/mnt/scratch/FA23_CS131_Ashish/hamzaf23/.local/bin:$PATH"
  598  source ~/.bashrc   # or source ~/.zshrc if you're using Zsh
  599  kaggle login
  600  export KAGGLE_USERNAME=hamzakhansjsu
  601  export KAGGLE_KEY=8a24a58f5bd43a51354fb2a7a66184d0
  602  kaggle login
  603  kaggle competitions list
  604  export KAGGLE_USERNAME=hamzakhansjsu
  605  export KAGGLE_KEY=8a24a58f5bd43a51354fb2a7a66184d0
  606  pip3 install --upgrade kaggle
  607  kaggle competitions list
  608  pip install kaggle
  609  chmod 600 ~/.kaggle/kaggle.json
  610  kaggle datasets download -d omarhanyy/imdb-top-1000
  611  unzip imdb-top-1000.zip
  612  ls
  613  cat 'IMDB top 1000.csv'
  614  ls
  615  cat IMDB%20top%201000.csv
  616  ls
  617  rm IMDB%20top%201000.csv
  618  ls
  619  cat 'IMDB top 1000.csv'
  620  ls
  621  cat  IMDB_top_1000.csv
  622  ls
  623  rm  IMDB_top_1000.csv
  624  ls
  625  cat imdb-top-1000.zip
  626  1;2c
  627  ls
  628  rm imdb-top-1000.zip
  629  ls
  630  cat 'login?titleType=dataset-downloads&showDatasetDownloadSkip=False&messageId=datasetsWelcome&returnUrl=%2Fdatasets%2Fomarhanyy%2Fimdb-top-1000?resource=download'
  631  ls
  632  rm 'login?titleType=dataset-downloads&showDatasetDownloadSkip=False&messageId=datasetsWelcome&returnUrl=%2Fdatasets%2Fomarhanyy%2Fimdb-top-1000?resource=download'
  633  ls
  634  cat topmovies.csv
  635  rm topmovies.csv
  636  ls
  637  cat 'IMDB top 1000.csv'
  638  history > cmds.log
  639  cd ..
  640  git add .
  641  git add MiniProjectGroup2
  642  git status
  643  cd MiniProjectGroup2
  644  git commit -m"added"
  645  git push origin master
  646  cd cs131
  647  ls
  648  cd MiniProjectGroup2
  649  wc -l "IMDB top 1000.csv"
  650  ls
  651  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | nl
  652  awk -F ',' 'NR>1 {for(i=1; i<=NF; i++) print $i}' "IMDB top 1000.csv" | sort -n | uniq -c
  653  # Identify average rating and most common genres
  654  awk -F ',' '{sum+=$5; count[$4]++} END {print "Average Rating:", sum/NR; print "Most Common Genres:"; for(genre in count) print genre, count[genre]}' "IMDB top 1000.csv"
  655  awk -F ',' 'NR>1 {for(i=1; i<=NF; i++) print $i}' "IMDB top 1000.csv" | sort -n | uniq -c
  656  cd cs131
  657  cd ​​ssh hamzaf23@172.20.25.9
  658  ls
  659  cd MiniProjectGroup2
  660  awk -F ',' 'NR>1 {for(i=1; i<=NF; i++) print "Column "i": "$i}' "IMDB top 1000.csv" | sort | uniq -c
  661  wc -l IMDb\ top\ 1000.csv
  662  head -n 1 'IMDB top 1000.csv'
  663  wc -l 'IMDB top 1000.csv'
  664  # Explore unique values in the 'Certificate' column
  665  awk -F',' '{print $2}' 'IMDB top 1000.csv' | sort | uniq -c
  666  # Calculate the average rating
  667  awk -F',' '{sum += $5} END {print "Average Rating:", sum/NR}' 'IMDB top 1000.csv'
  668  awk -F',' '{print $2}' 'IMDB top 1000.csv'
  669  awk -F',' '{print $1}' 'IMDB top 1000.csv'
  670  awk -F',' '{print $3}' 'IMDB top 1000.csv'
  671  awk -F',' '{print $2}' 'IMDB top 1000.csv' 
  672  awk -F',' '{print $3}' 'IMDB top 1000.csv' | sort | uniq -c
  673  awk -F',' '{print $2}' 'IMDB top 1000.csv' | sort | uniq -c
  674  awk -F',' '{print $1, $2, $3, $4}' 'IMDB top 1000.csv' | sort | uniq -c
  675  awk -F',' '{print $3, $4}' 'IMDB top 1000.csv' | sort | uniq -c
  676  awk -F',' '{print $2, $3, $4}' 'IMDB top 1000.csv' | sort | uniq -c
  677  awk -F',' '{sum += $5} END {print "Average Rating:", sum/NR}' 'IMDB top 1000.csv'
  678  awk -F',' '{sum += $4} END {print "Average Rating:", sum/NR}' 'IMDB top 1000.csv'
  679  awk -F',' '{sum += $5} END {print "Average Rating:", sum/NR}' 'IMDB top 1000.csv'
  680  awk -F',' '{sum += $4} END {printawk -F',' '{print $2, $3, $4}' 'IMDB top 1000.csv' | sort | uniq -c "Average Rating:", sum/NR}' 'IMDB top 1000.csv'
  681  awk -F',' '{print $2, $3}' 'IMDB top 1000.csv' | sort | uniq -c
  682  awk -F',' '{print $2, $3, $4}' 'IMDB top 1000.csv' | sort
  683  awk -F',' '{print $5}' 'IMDB top 1000.csv' | sort
  684  awk -F',' '{print $6}' 'IMDB top 1000.csv' | sort
  685  awk -F',' '{print $7}' 'IMDB top 1000.csv' | sort
  686  awk -F',' '{print $7}' 'IMDB top 1000.csv'
  687  awk -F',' '{print $6}' 'IMDB top 1000.csv'
  688  awk -F',' '{print $1 $2 $3 $4 $5 $6 $7 $8}' 'IMDB top 1000.csv' | sort
  689  awk -F',' '{print $8}' 'IMDB top 1000.csv' | sort
  690  awk -F',' '{print $7}' 'IMDB top 1000.csv' | sort
  691  awk -F',' '{print $5}' "IMDB top 1000.csv"
  692  awk -F',' '{gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv"
  693  awk -F',' '{print $0}' "IMDB top 1000.csv"
  694  awk -F',' '{print $9}' "IMDB top 1000.csv"
  695  awk -F',' '{print $5}' "IMDB top 1000.csv"
  696  cut -d',' -f5 "IMDB top 1000.csv" | awk -F'"' '{sum+=$1; count+=1} END {print "Average Rating:", sum/count}'
  697  cut -d',' -f5 "IMDB top 1000.csv"
  698  cut -d',' -f1 "IMDB top 1000.csv"
  699  cut -d',' -f6 "IMDB top 1000.csv"
  700  cut -d',' -f7 "IMDB top 1000.csv"
  701  cut -d',' -f "IMDB top 1000.csv"
  702  cut -d',' -f5 "IMDB top 1000.csv"
  703  cut -d',' -f5 "IMDB top 1000.csv" | grep -oP '\d+\.\d+' 
  704  cut -d',' -f5 "IMDB top 1000.csv" | grep -oP '\d+\.\d+' | awk '{ sum += $1 } END { if (NR > 0) print "Average Rating:", sum/NR; else print "No Ratings found" }'
  705  cut -d',' -f5 'IMDB top 1000.csv' | grep -oP '\d+\.\d+' 
  706  cut -d',' -f5 'IMDB top 1000.csv' | grep -oP '\d+\.\d+' | awk '{ sum += $1 } END { if (NR > 0) print "Average Rating:", sum/NR; else print "No Ratings found" }'
  707  $1
  708  print $1
  709  csvcut -c "Rate" "IMDB top 1000.csv"
  710  cut -d',' -f5 "IMDB top 1000.csv" | grep -oP '\d+\.\d+' | awk '{ sum += $1 } END { if (NR > 0) print "Average Rating:", sum/NR; else print "No Ratings found" }'
  711  cut -d ',' -f5 'IMDB top 1000.csv' | awk '{ print $5}'
  712  awk -F',' '{print $5}' "IMDB top 1000.csv"
  713  grep -Eo '"[0-9]+(\.[0-9]+)?"' "IMDB top 1000.csv"
  714  awk -F'","' '{print $5}' "IMDB top 1000.csv" | sed 's/"//g'
  715  csvcut -n "IMDB top 1000.csv"
  716  csvcut -c 5 "IMDB top 1000.csv" | sed 's/"//g'
  717  awk -F, '{gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv"
  718  awk -F, '{gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv"
  719  ction
  720  Action
  721  Drama
  722  Biography
  723  Action
  724  Drama
  725  Action
  726  Biography
  727  Drama
  728  Action
  729  Drama
  730  Action
  731  Drama
  732  Action
  733  Drama
  734  Action
  735  Animation
  736  Biography
  737  Adventure
  738  Action
  739  Crime
  740  Biography
  741  Adventure
  742  Animation
  743  Action
  744  Drama
  745   Winter... and Spring (2003)
  746  Biography
  747  Comedy
  748  Action
  749  Adventure
  750  Animation
  751  Comedy
  752  Crime
  753  Comedy
  754  Drama
  755  92 min
  756  Drama
  757  Crime
  758  Drama
  759  Biography
  760  Animation
  761  Drama
  762  Animation
  763  Action
  764  Drama
  765  Comedy
  766  Mystery
  767  Animation
  768  Comedy
  769  180 min
  770  Actionawk -F, '{gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv"
  771  awk -F, '{gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv"
  772  awk -F, '{gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv"
  773  awk -F, 'NR > 1 && $6 ~ /^[0-9]+(\.[0-9]+)?$/ {gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv"
  774  awk -F ',' '{print $6}' "IMDB top 1000.csv" | grep -oE '[0-9]+(\.[0-9]+)?'
  775  awk -F ',' '{print $6}' "IMDB top 1000.csv" | grep -oE '[0-9]+(\.[0-9]+)?' > ratings.txt
  776  cat ratings.txt
  777  awk -F ',' 'NR > 1 {gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv" | grep -oE '[0-9]+(\.[0-9]+)?' | sort | uniq -c
  778  # Assuming your dataset is in CSV format
  779  wc -l "IMDB top 1000.csv"
  780  # Assuming your dataset is in CSV format
  781  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | wc -l
  782  awk -F ',' 'NR > 1 && $6 != "" {gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv" | grep -oE '[0-9]+(\.[0-9]+)?'
  783  awk -F ',' 'NR > 1 && $6 ~ /^[0-9]+(\.[0-9]+)?$/ {gsub(/"/, "", $6); print $6}' "IMDB top 1000.csv"
  784  awk -F ',' 'NR > 1 {gsub(/"/, "", $6); if($6 == "") print "Empty rate at line " NR; print $6}' "IMDB top 1000.csv"
  785  awk -F ','  {print $6}' "IMDB top 1000.csv"
  786  awk -F ',' '{print $6}' 'IMDb top 1000.csv'
  787  awk -F ',' '{print $6}' 'IMDB top 1000.csv'
  788  awk -F ',' '{print $7}' 'IMDB top 1000.csv'
  789  awk -F ',' '{print $1}' 'IMDB top 1000.csv'
  790  awk -F ',' '{print $3}' 'IMDB top 1000.csv'
  791  awk -F ',' '{print $4}' 'IMDB top 1000.csv'
  792  awk -F ',' '{print $6}' 'IMDB top 1000.csv'
  793  cat 'IMDB top 1000.csv'
  794  awk -F ',' '{print $5}' 'IMDB top 1000.csv'
  795  head "IMDB top 1000.csv"
  796  awk -F ',' 'NR > 1 {gsub(/"/, ""); print $7}' "IMDB top 1000.csv"
  797  awk -F ',' 'NR > 1 {gsub(/"/, ""); print $6}' "IMDB top 1000.csv"
  798  awk -F ',' 'NR > 1 {gsub(/"/, ""); print $6}' "IMDB top 1000.csv" | grep -oE '[0-9]+(\.[0-9]+)?'
  799  pip install csvkit
  800  sudo apt-get update
  801  python3 -m venv myenv
  802  source myenv/bin/activate
  803  pip install csvkit
  804  python3
  805  cd..
  806  cd cs131
  807  (myenv) [hamzaf23@sjsu MiniProjectGroup2]$ deactivate
  808  deactivate
  809  csvcut -c "Rate" "IMDB top 1000.csv" | tail -n +2
  810  pip install csvkit
  811  sudo apt-get update
  812  python3 -m pip install --user csvkit
  813  pip install csvkit
  814  python3 -m pip install --user csvkit
  815  csvcut -n "IMDB top 1000.csv"
  816  python3 --version
  817  python3 -c 'import csv; with open("IMDB top 1000.csv", "r") as file: data = csv.reader(file); print(next(data))'
  818  wc -l "IMDB top 1000.csv" > numberofentries.txt
  819  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' > featurescolumnames.txt
  820  csvcut -n "IMDB top 1000.csv"
  821  import matplotlib.pyplot as plt 
  822  python
  823  python3
  824  python3 script.py
  825  python script.py
  826  script.py
  827  sudo apt update
  828  python3 --version
  829  python3 visualize_data.py
  830  ls
  831  cd cs131
  832  ls
  833  cd MiniProjectGroup2
  834  chmod +x visualize_data.py
  835  ls visualize_data.py
  836  nano visualize_data.py
  837  ls
  838  vi vd.py
  839  ls
  840  chmod +x visualize_data.py
  841  chmod +x vd.py
  842  python3 visualize_data.py
  843  python3 vd.py
  844  vi vd.py
  845  fg
  846  cd cs131
  847  ls
  848  cd MiniProjectGroup2
  849  ls
  850  vi vd.py
  851  python3 vd.py
  852  vi vd.py
  853  python3 vd.py
  854  install pandas
  855  pip install pandas
  856  pip3 install pandas
  857  pip install --upgrade pip
  858  pip3 install pandas
  859  sudo apt-get install python3-dev
  860  pip3 install --user pandas
  861  pip3 install --user matplotlib
  862  sudo apt-get install python3-dev
  863  pip3 install --user python3-dev
  864  pip3 install --user pandas
  865  pip3 install --user numpy
  866  pip3 install --user numpy pandas
  867  pip3 install --user --no-build-isolation numpy pandas
  868  pip3 install --user --no-build-isolation tomli
  869  pip3 install --user --no-build-isolation numpy pandas
  870  pip3 install --user --no-build-isolation tomli
  871  pip3 install --user --no-build-isolation numpy pandas
  872  pip3 install --user --no-build-isolation packaging
  873  pip3 install --user --no-build-isolation numpy pandas
  874  brew install pandas
  875  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  876  brew install pandas
  877  vi test1.py
  878  python3 test1.py
  879  cd cs131
  880  ls
  881  cd MiniProjectGroup2
  882  pip install kaggle
  883  pip3 install --upgrade kaggle
  884  pip3 install --user numpy
  885  sudo yum install python3-devel
  886  su -
  887  pip3 install --user numpy pandas
  888  python3 -m venv myenv
  889  source myenv/bin/activate
  890  pip install numpy pandas
  891  source myenv/bin/activate
  892  pip install numpy pandas
  893  sudo yum install python3-dev
  894  source myenv/bin/activate
  895  pip install numpy pandas
  896  -m pip install --upgrade pip
  897  pip install --upgrade pip
  898  source myenv/bin/activate
  899  pip install numpy pandas
  900  deactivate
  901  ls
  902  python3 vd.py
  903  vi vd.py
  904  python3 vd.py
  905  source myenv/bin/activate
  906  pip install pandas
  907  deactivate
  908  # Example: Creating a Bar Chart with gnuplot
  909  echo -e "Action 10\nDrama 20\nComedy 15" | gnuplot -p -e "set title 'Movie Genre Distribution'; set style fill solid; set boxwidth 0.5; set terminal dumb; plot '-' using 2:xticlabels(1) with boxes"
  910  wc 'animation' 'IMDB top 1000.csv
  911  wc 'animation' 'IMDB top 1000.csv'
  912  wc "animation" 'IMDB top 1000.csv
  913  wc "animation" 'IMDB top 1000.csv'
  914  wc "animation" 'IMDB top 1000.csv'
  915  grep -o -i 'animation' your_file.csv | wc -l
  916  grep -o -i 'animation' 'IMDB top 1000.csv' | wc -l
  917  grep -o -i 'drama' 'IMDB top 1000.csv' | wc -l
  918  grep -o -i 'crime' 'IMDB top 1000.csv' | wc -l
  919  grep 'crime' 'IMDB top 1000.csv
  920  grep 'crime' 'IMDB top 1000.csv' | wc -l
  921  grep -o 'crime' 'IMDB top 1000.csv' | wc -l 
  922  grep -o -i 'crime' 'IMDB top 1000.csv' | wc -l
  923  grep -o -i 'Crime' 'IMDB top 1000.csv' | wc -l
  924  grep -o -i 'mystery' 'IMDB top 1000.csv' | wc -l
  925  grep -o -i 'biography' 'IMDB top 1000.csv' | wc -l
  926  grep -o -i 'romance' 'IMDB top 1000.csv' | wc -l
  927  grep -o -i 'animation' 'IMDB top 1000.csv' | wc -l
  928  grep -o -i 'crime' 'IMDB top 1000.csv' | wc -l
  929  grep -o -i 'mystery' 'IMDB top 1000.csv' | wc -l
  930  grep -o -i 'biography' 'IMDB top 1000.csv' | wc -l
  931  grep -o -i 'thriller' 'IMDB top 1000.csv' | wc -l
  932  grep -o -i 'crime' 'IMDB top 1000.csv' | wc -l
  933  grep -o -i 'action' 'IMDB top 1000.csv' | wc -l
  934  grep -o -i 'comedy' 'IMDB top 1000.csv' | wc -l
  935  grep -o -i 'adventure' 'IMDB top 1000.csv' | wc -l
  936  grep -o -i 'drama' 'IMDB top 1000.csv' | wc -l
  937  num_entries=$(wc -l < "IMDB top 1000.csv")
  938  echo "Number of entries: $num_entries"
  939  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | awk '{print "Feature: " $1}'
  940  value_ranges=$(awk -F',' 'NR>1 {print $4}' "IMDB top 1000.csv" | sort | uniq)
  941  echo "Value ranges for Genre: $value_ranges"
  942  value_ranges=$(awk -F',' 'NR>1 {print $5}' "IMDB top 1000.csv" | sort | uniq)
  943  echo "Value ranges for Genre: $value_ranges"
  944  $4
  945  awk '{print $4}'
  946  awk -F',' ' {print $5}' "IMDB top 1000.csv" | sort | uniq
  947  awk -F',' ' {print $5}' "IMDB top 1000.csv" | sort
  948  awk -F',' ' {print $4}' "IMDB top 1000.csv" | sort
  949  value_ranges=$(awk -F',' 'NR>1 {print $4}' "IMDB top 1000.csv" | sort | uniq)
  950  echo "Value ranges for Genre: $value_ranges"
  951  genre_ranges=$(awk -F',' 'NR>1 {print $4}' "IMDB top 1000.csv" | sort | uniq)
  952  echo "Value ranges for Genre: $genre_ranges"
  953  head -n 5 "IMDB top 1000.csv" | cat -n
  954  genre_ranges=$(awk -F',' 'NR>1 {print $5}' "IMDB top 1000.csv" | sort | uniq)
  955  echo "Value ranges for Genre: $genre_ranges"
  956  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | sort | uniq)
  957  echo "Value ranges for Genre: $genre_ranges"
  958  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | grep -v '[0-9]' | sort | uniq)
  959  echo "Value ranges for Genre: $genre_ranges"
  960  grep -o -i 'adventure' 'IMDB top 1000.csv' | wc -l
  961  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | grep -vE '[0-9]|Winter\.\.\. and Spring' | sort | uniq)
  962  echo "Value ranges for Genre: $genre_ranges"
  963  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | grep -vE '[0-10]|Winter\.\.\. and Spring' | sort | uniq)
  964  echo "Value ranges for Genre: $genre_ranges"
  965  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | grep -vE '[0-9]|Winter\.\.\. and Spring' | sort | uniq)
  966  echo "Value ranges for Genre: $genre_ranges"
  967  awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | sort | uniq
  968  genre_ranges=$(awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | grep -vE '[0-9]|Winter\.\.\. and Spring' | sort | uniq)
  969  echo "Value ranges for Genre: $genre_ranges"
  970  awk -F',' 'NR>1 {gsub(/"/, "", $5); if ($5 !~ /^[0-9]/) print $5}' "IMDB top 1000.csv" | sort | uniq
  971  cd cs131
  972  ;s
  973  ls
  974  cd MiniProjectGroup2
  975  awk -F',' 'NR>1 {gsub(/"/, "", $5); print $5}' "IMDB top 1000.csv" | sort | uniq
  976  awk -F',' 'NR>1 {gsub(/"/, "", $5); if ($5 !~ /^[0-9]/) print $5}' "IMDB top 1000.csv" | sort | uniq
  977  awk -F',' 'NR>1 {gsub(/"/, "", $5); if ($5 !~ /^[0-9]/) genres[$5]} END {for (genre in genres) print genre}' "IMDB top 1000.csv"
  978  awk -F',' 'NR>1 {gsub(/"/, "", $5); print gensub(/^[0-9]+ /, "", "g", $5)}' "IMDB top 1000.csv" | sort | uniq
  979  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) print gensub(/^[0-9]+ /, "", "g", genres[i])}' "IMDB top 1000.csv" | sort | uniq
  980  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) print gensub(/^[0-9]+ /, "", "g", genres[i])}' "IMDB top 1000.csv" | sort 
  981  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, /); for (i in genres) print genres[i]}' "IMDB top 1000.csv" | sort | uniq
  982  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); print $5}' "IMDB top 1000.csv" | tr -d '"' | sort | uniq
  983  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); print $5}' "IMDB top 1000.csv" | tr -d '"' | sort
  984  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); print $5}' "IMDB top 1000.csv" | tr -d '"' | sort | uniq
  985  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); gsub(/[^a-zA-Z, -]/, "", $5); gsub(/,/, "\n", $5); print $5}' "IMDB top 1000.csv" | sort | uniq
  986  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); gsub(/[^a-zA-Z, -]/, "", $5); gsub(/,[^a-zA-Z]/, "\n", $5); gsub(/[^a-zA-Z],/, "\n", $5); print $5}' "IMDB top 1000.csv" | sort | uniq
  987  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, "\n", $5); gsub(/[^a-zA-Z, -]/, "", $5); gsub(/,[^a-zA-Z]/, "\n", $5); gsub(/[^a-zA-Z],/, "\n", $5); if ($5 !~ /min|Winter and Spring/) print $5}' "IMDB top 1000.csv" | sort | uniq
  988  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/[^a-zA-Z, -]/, "", $5); gsub(/,/, "\n", $5); gsub(/^ */, "", $5); if ($5 !~ /[0-9]/) print $5}' "IMDB top 1000.csv" | sort | uniq
  989  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) print genres[i]}' "IMDB top 1000.csv" | sort | uniq
  990  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "") print genre }}' "IMDB top 1000.csv" | sort | uniq
  991  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "") print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr | head -n 20
  992  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+ min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr | head -n 20
  993  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+ min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr | head -n 20
  994  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+ min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sed 's/^[ \t]*//;s/[ \t]*$//' | sort | uniq -c | sort -nr | head -n 20
  995  awk -F',' 'NR>1 {gsub(/"/, "", $5); split($5, genres, /, */); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+ min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | tr -d '[:space:]' | sort | uniq -c | sort -nr | head -n 20
  996  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, ","); gsub(/ /, ""); split($5, genres, ","); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr | head -n 20
  997  awk -F',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, ","); gsub(/ /, ""); split($5, genres, ","); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr
  998  cut -d ',' -f 2 data.csv | sort | uniq -c
  999  cut -d ',' -f 4 'IMDB top 1000.csv' | sort | uniq -c
 1000  cut -d ',' -f 5 'IMDB top 1000.csv' | sort | uniq -c
 1001  cut -d ',' -f 5 'IMDB top 1000.csv' | sort | uniq
 1002  cut -d ',' -f 6 'IMDB top 1000.csv' | sort | uniq
 1003  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($5 "," $6, genres, ","); for (i in genres) { genre = tolower(genres[i]); if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr
 1004  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq -c | sort -nr
 1005  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr
 1006  awk -F ',' '{print $5, $6, $7}' 'IMDB top 1000.csv'
 1007  awk -F ',' '{print $5, $6, $7}' 'IMDB top 1000.csv' | tr -d '"' | tr ',' '\n' | sort -u
 1008  awk -F ',' '{print $5, $6, $7}' 'IMDB top 1000.csv' | tr -d '"' | tr ',' '\n' | grep -E -v '^[0-9]+(\.[0-9]+)?$' | grep -v 'Winter... and Spring (2003) R 103 min' | sort -u
 1009  awk -F ',' '!/^[0-9]+(\.[0-9]+)?$|Winter\.\.\. and Spring \(2003\) R 103 min/{gsub(/"/, ""); gsub(/, /, ","); split($5, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "") { gsub(/[0-9]+min/, "", genre); print tolower(genre) }}}' 'IMDB top 1000.csv' | sort -u
 1010  echo "Number of Entries:"
 1011  wc -l imdb_dataset.csv | awk '{print $1}'echo "Number of Entries:"
 1012  wc -l imdb_dataset.csv | awk '{print $1}'awk 'NR>1 {count++} END {print "Number of Entries: " count}' 'IMDB top 1000.csv' > entries.txt
 1013  awk 'NR>1 {count++} END {print "Number of Entries: " count}' 'IMDB top 1000.csv' > entries.txt
 1014  cat entries.txt
 1015  awk -F ',' '{print $5}' 'IMDB top 1000.csv' | tr -d '"' | tr ',' '\n' | sort -u | grep -v '^$' 
 1016  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | awk '{print "Feature: " $1}'
 1017  num_entries=$(wc -l < "IMDB top 1000.csv")
 1018  echo "Number of entries: $num_entries"
 1019  ls
 1020  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | awk '{print "Feature: " $1}'
 1021  head -n 1 "IMDB top 1000.csv" | tr ',' '\n' | awk '{print "Feature: " $1}' > Features.txt
 1022  # Extract and sort unique values for the "genre" feature
 1023  awk -F ',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, ","); gsub(/ /, ""); split($5, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' 'IMDB top 1000.csv' | sort -u > genre_values.txt
 1024  awk -F ',' 'NR>1 {gsub(/"/, "", $5); gsub(/, /, ","); gsub(/ /, ""); split($5, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/) print genre }}' 'IMDB top 1000.csv' | sort -u
 1025  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved)$/ && length(genre) > 1) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr
 1026  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved|TV-PG|TV-MA|TV-14)$/ && length(genre) > 1) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr
 1027  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved|TV-PG|TV-MA|TV-14|PG-13|PG|Summer)$/ && length(genre) > 1) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr
 1028  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved|TV-PG|TV-MA|TV-14|PG-13|PG|Summer)$/ && length(genre) > 1 && genre !~ /^[0-9.]+$/) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr
 1029  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved|TV-PG|TV-MA|TV-14|PG-13|PG|Summer)$/ && length(genre) > 1 && genre !~ /^[0-9.]+$/) print genre }}' "IMDB top 1000.csv" | sort | uniq | sort -nr > valuerange1.txt
 1030  vi ourscript.sh
 1031  cat valuerange1.txt
 1032  vi outscrip.sh
 1033  vi ourscript.sh
 1034  chmod +x myscript.sh
 1035  chmod +x ourscript.sh
 1036  ./ourscript.sh
 1037  vi ourscript.sh
 1038  ./ourscript.sh
 1039  ls
 1040  cat ratings.txt
 1041  ls
 1042  cat valuerange1.txt
 1043  ls
 1044  cat Features.txt
 1045  cat feature.txt
 1046  cat ourscript.sh
 1047  ls
 1048  cat feature.txt
 1049  cat entries.txt
 1050  awk -F',' 'NR>1 {gsub(/"/, ""); print $12}' "IMDB top 1000.csv" | awk 'NF' | sort | uniq -c | sort -nr
 1051  awk -F',' 'NR>1 {gsub(/"/, ""); gsub(/, /, ","); gsub(/ /, ""); split($3 "," $5 "," $6 "," $7, genres, ","); for (i in genres) { genre = genres[i]; if (genre != "" && genre !~ /^[0-9]+$/ && genre !~ /^[0-9]+min$/ && genre !~ /\(/ && genre !~ /^(Passed|NotRated|GP|G|Approved|TV-PG|TV-MA|TV-14|PG-13|PG|Summer)$/ && length(genre) > 1) print genre "," $8 }}' "IMDB top 1000.csv" | awk -F',' '{ sum[$1] += $2; count[$1]++ } END { for (genre in sum) print genre "," sum[genre] / count[genre] }' | sort -t, -k2,2nr
 1052  ls
 1053  cat valuerange1.txt
 1054  history > cmds.log
